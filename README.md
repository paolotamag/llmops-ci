# Adopting LLMOps in Continous Integration (CI) Tests

When deploying LLM calls on raw conversation data, it is paramount to programmatically verify that the LLM calls are still returning expected results, following the principles of _continous integration (CI)_. 

In this repository: 
- I demonstrate how to set this up using [Langfuse](https://langfuse.com/docs/datasets/overview) (open source) and [GitHub Actions](https://github.com/features/actions). 
- I provide a small dataset (pet food customer service) was generated by an LLM.
- I provide a small LLM process using intent detection to trigger simple outcomes (like for example pre-set answers). This is a common approach in customer service, which got re-adapted recently to levarage LLMs capabilities in several papers, for example [Arora et al (2024)](https://www.amazon.science/publications/intent-detection-in-the-age-of-llms).
- A notebook to set up Langfuse datasets.
- A script to run a test using a Langfuse experiment (on the created dataset).
- A github workflow (.yml) to set up GitHub Actions.
- A guide (below) on how to proceed step by step.

## GitHub Actions Setup Guide for Langfuse Experiments

This guide walks you through setting up GitHub Actions to automatically run Langfuse experiments using Python.

### ğŸ“‹ Overview

This setup allows you to:

- Run Langfuse experiments automatically on a schedule
- Trigger experiments manually with custom parameters
- Monitor success rates and get notifications
- Store experiment results as artifacts

### ğŸš€ Quick Start

#### 1. Project Structure

Your repository should have this structure:

```
langfuse-ci/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ langfuse-ci.yml
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ api_runner.py
â”œâ”€â”€ model/
â”‚   â””â”€â”€ run_model.py         # Your model implementation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                     # For local development
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

#### 2. Required Files

The main files you need are:

- `api_runner.py` - Main experiment runner script
- `langfuse-ci.yml` - GitHub Actions workflow
- `requirements.txt` - Python dependencies
- `.env` - Local environment variables (not committed)

#### 3. Set Up GitHub Secrets

1. Go to your GitHub repository
2. Navigate to Settings â†’ Secrets and variables â†’ Actions
3. Add these secrets:

| Secret Name | Description |
|-------------|-------------|
| `LANGFUSE_SECRET_KEY` | Your Langfuse secret key |
| `LANGFUSE_PUBLIC_KEY` | Your Langfuse public key |
| `LANGFUSE_HOST` | Langfuse host URL |
| `OPEN_AI_KEY` | Your OpenAI API key |

#### 4. Create `.env` for Local Development

Create `.env` in your project root (add to `.gitignore`):

```env
LANGFUSE_SECRET_KEY=your_secret_key_here
LANGFUSE_PUBLIC_KEY=your_public_key_here
LANGFUSE_HOST=https://cloud.langfuse.com
EXPERIMENT_NAME=test_reproducibility
SUCCESS_THRESHOLD=0.8
```

### ğŸ¯ Usage

#### Automatic Runs

- Experiments run daily at 9 AM UTC
- Triggered on pushes to main branch when code changes

#### Manual Runs

1. Go to Actions tab in GitHub
2. Select "ğŸ§ª Langfuse Experiment Runner"
3. Click "Run workflow"
4. Set parameters:
   - **Experiment name**: Name for your experiment
   - **Success threshold**: Minimum success rate (0.0-1.0)
   - **Log level**: info/debug/warning

#### Local Testing

```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export LANGFUSE_SECRET_KEY="your_key"
export LANGFUSE_PUBLIC_KEY="your_key" 
export LANGFUSE_HOST="https://cloud.langfuse.com"
export OPENAI_KEY = "your_key"

# Run experiment
python scripts/api_runner.py
```

If you run in import issues set your `PYTHONPATH` in the terminal to the root of the repo.

### ğŸ“Š Monitoring

#### Success Indicators

- âœ… Workflow completes successfully
- âœ… Success rate meets or exceeds threshold
- âœ… Results saved as artifacts

#### Failure Notifications

- ğŸš¨ Automatic GitHub issue creation on failure
- âš ï¸ Issues created when success rate below threshold
- ğŸ“§ Include experiment results and links to logs

#### Artifacts

- Experiment results saved as JSON files
- Available for download for 30 days
- Include success rates, timestamps, and metadata
