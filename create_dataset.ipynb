{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f81a7b",
   "metadata": {},
   "source": [
    "# Langfuse Dataset Creation and Experimentation\n",
    "\n",
    "This notebook demonstrates how to create a dataset in Langfuse and run experiments to evaluate model performance against known answers for a pet food customer service use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f5b5c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Load environment variables (API keys, configuration) from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1570343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446e4c1",
   "metadata": {},
   "source": [
    "## 2. Test the Model with Sample Questions\n",
    "\n",
    "Before creating the dataset, let's test our model with some sample customer questions to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.run_model import run_model\n",
    "\n",
    "import json\n",
    "\n",
    "# Method 1: Read from a file\n",
    "with open('example_data/prepared_answers_pet_food.json', 'r') as file:\n",
    "    prepared_answers = json.load(file)\n",
    "\n",
    "\n",
    "# Example customer questions\n",
    "test_questions = [\n",
    "    \"My dog has been having stomach issues after eating your kibble\",\n",
    "    \"When will my order #12345 be delivered?\",\n",
    "    \"What's the best food for a senior Golden Retriever?\",\n",
    "    \"I want to return this bag of food\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "\n",
    "    print(run_model(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315b6a0",
   "metadata": {},
   "source": [
    "## 3. Initialize Langfuse Client\n",
    "\n",
    "Create a connection to Langfuse for dataset management and experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea220a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    " \n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c056a",
   "metadata": {},
   "source": [
    "## 4. Create the Dataset\n",
    "\n",
    "Create a new dataset in Langfuse with metadata describing the purpose and structure of our evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6607c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"pet_food_kwnown_answers\",\n",
    "    # optional description\n",
    "    description=\"example dataset with customer questions with known answers\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"Paolo Tamagnini\",\n",
    "        \"date\": \"2025-06-28\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b94d6",
   "metadata": {},
   "source": [
    "## 5. Load Dataset and Prepared Answers\n",
    "\n",
    "Load the raw dataset containing customer questions and the corresponding prepared answers that serve as ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read from file\n",
    "with open('example_data/pet_food.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Read from file\n",
    "with open('example_data/prepared_answers_pet_food.json', 'r') as file:\n",
    "    prepared_answers = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0918589",
   "metadata": {},
   "source": [
    "## 6. Populate Dataset Items\n",
    "\n",
    "Create individual dataset items in Langfuse, each containing:\n",
    "- Input: Customer question\n",
    "- Expected output: The correct prepared answer ID and text\n",
    "- Metadata: Additional context like customer info, pet details, and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=\"pet_food_kwnown_answers\",\n",
    "        # any python object or value, optional\n",
    "        input={\n",
    "             \"question\": item[\"message\"],\n",
    "        },\n",
    "        # any python object or value, optional\n",
    "        expected_output={\n",
    "            \"prepared_answer_id\": item[\"answer_id\"],\n",
    "            \"prepared_answer_text\": prepared_answers[str(item[\"answer_id\"])]\n",
    "        },\n",
    "        # metadata, optional\n",
    "        metadata={\n",
    "            \"model\": \"Pet Food Customer Service\",\n",
    "            'customer_name': item['customer_name'],\n",
    "            'pet_type': item['pet_type'],\n",
    "            'pet_name': item['pet_name'],\n",
    "            'category': item['category'],\n",
    "            'timestamp': item['timestamp'],\n",
    "            'response_timestamp': item['response_timestamp'],\n",
    "            'status': item['status'],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d319fa5",
   "metadata": {},
   "source": [
    "## 7. Define Experiment Functions\n",
    "\n",
    "Create functions to:\n",
    "- Compare model outputs with expected answers\n",
    "- Run experiments across the entire dataset\n",
    "- Track success metrics and individual trace scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_prepared_answer_ids(output, expected_output):\n",
    "  if int(output[\"prepared_answer_id\"]) == expected_output[\"prepared_answer_id\"]:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "# experiment_name = \"test_reproducibility\"\n",
    "def run_experiment(experiment_name):\n",
    "    dataset = langfuse.get_dataset(\"pet_food_kwnown_answers\")\n",
    "\n",
    "    items_number = len(dataset.items)\n",
    "    success_count = 0\n",
    "    for item in dataset.items:\n",
    "\n",
    "        # Use the item.run() context manager\n",
    "        with item.run(\n",
    "            run_name = experiment_name,\n",
    "\n",
    "        ) as root_span: # root_span is the root span of the new trace for this item and run.\n",
    "            # All subsequent langfuse operations within this block are part of this trace.\n",
    "\n",
    "            # Call your application logic\n",
    "            output = run_model(item.input[\"question\"])\n",
    "\n",
    "\n",
    "            comparison_result = compare_prepared_answer_ids(output, item.expected_output)\n",
    "            success_count += comparison_result\n",
    "\n",
    "            # Optionally, score the result against the expected output\n",
    "            root_span.score_trace(name=\"prepared_answer_id\", value = comparison_result)\n",
    "\n",
    "    success_metric = success_count/items_number\n",
    "    print(f\"\\nFinished processing dataset 'Pet Food Customer Service' for run '{experiment_name}'.\")\n",
    "    print(f\"Success rate: {success_count}/{items_number} ({(success_count/items_number)*100:.2f}%)\")\n",
    "\n",
    "    return success_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9f151",
   "metadata": {},
   "source": [
    "## 8. Run the Experiment\n",
    "\n",
    "Execute the experiment to evaluate model performance against the dataset, measuring how often the model selects the correct prepared answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from model.run_model import run_model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "langfuse = Langfuse()\n",
    "\n",
    "run_experiment(\"test_reproducibility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
